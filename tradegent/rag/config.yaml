rag_version: "2.0.0"
embed_dims: "${EMBED_DIMENSIONS:-1536}"

database:
  url: "${DATABASE_URL:-postgresql://lightrag:lightrag@localhost:5433/lightrag}"
  schema: "${RAG_SCHEMA:-nexus}"

# Feature flags for new RAG capabilities
features:
  # Phase 1: Metrics collection
  metrics_enabled: "${RAG_METRICS:-true}"

  # Phase 2: Improved chunking
  element_aware_chunking: "${RAG_ELEMENT_CHUNKING:-true}"
  preserve_tables: "${RAG_PRESERVE_TABLES:-true}"

  # Phase 3: Cross-encoder reranking
  reranking_enabled: "${RAG_RERANKING:-true}"

  # Phase 4: Query classification for adaptive routing
  adaptive_retrieval: "${RAG_ADAPTIVE:-true}"

  # Phase 6: Query expansion
  query_expansion_enabled: "${RAG_QUERY_EXPANSION:-true}"

  # Phase 8: Semantic chunking (experimental)
  semantic_chunking: "${RAG_SEMANTIC_CHUNKING:-false}"

embedding:
  # EMBED_PROVIDER allows separate provider for embeddings (should stay consistent!)
  # Falls back to LLM_PROVIDER for backwards compatibility
  default_provider: "${EMBED_PROVIDER:-ollama}"
  fallback_chain:
    - ollama
    - openrouter
  timeout_seconds: "${EMBED_TIMEOUT_SECONDS:-30}"
  dimensions: "${EMBED_DIMENSIONS:-1536}"

  # Unified LLM config - provider selected by LLM_PROVIDER
  ollama:
    base_url: "${LLM_BASE_URL:-http://localhost:11434}"
    model: "${EMBED_MODEL:-nomic-embed-text}"

  openrouter:
    api_key: "${LLM_API_KEY}"
    model: "${EMBED_MODEL:-openai/text-embedding-3-small}"
    dimensions: "${EMBED_DIMENSIONS:-1536}"

  openai:
    api_key: "${OPENAI_API_KEY}"
    model: "${EMBED_MODEL:-text-embedding-3-large}"
    dimensions: "${EMBED_DIMENSIONS:-1536}"

chunking:
  # Optimized based on arXiv 2402.05131 research
  # 512-1024 tokens with 100-300 overlap gives 15-25% accuracy gain
  max_tokens: "${CHUNK_MAX_TOKENS:-768}"
  min_tokens: "${CHUNK_MIN_TOKENS:-50}"
  overlap_tokens: "${CHUNK_OVERLAP:-150}"

reranking:
  # Cross-encoder model for two-stage retrieve-then-rerank
  model: "${RERANK_MODEL:-cross-encoder/ms-marco-MiniLM-L-6-v2}"
  # Number of candidates to retrieve before reranking
  retrieval_k: "${RERANK_RETRIEVAL_K:-50}"

query_expansion:
  # Number of semantic variations to generate
  n_expansions: "${QUERY_EXPANSION_N:-3}"
  # Include original query in expansion set
  include_original: true
  # LLM model for generating variations
  model: "${QUERY_EXPANSION_MODEL:-gpt-4o-mini}"

semantic_chunking:
  # Minimum cosine similarity to keep sentences in same chunk
  similarity_threshold: "${SEMANTIC_CHUNK_THRESHOLD:-0.8}"

# LLM Generation Settings (for RAG answer generation)
generation:
  model: "${LLM_MODEL:-llama3.2}"
  temperature: "${LLM_TEMPERATURE:-0.1}"
  num_predict: "${LLM_NUM_PREDICT:-200}"
  max_tokens: "${LLM_MAX_TOKENS:-200}"
  top_p: "${LLM_TOP_P:-0.9}"
  top_k: "${LLM_TOP_K:-40}"

logging:
  embed_log: "logs/rag_embed.jsonl"
  metrics_log: "logs/rag_metrics.jsonl"
